67.11416066539957 % for 10k epochs, window adjusted to 8, 32 neurons per layer for 2 layers and 0.05 learning rate. 
Took around 6.5hrs to run the training for this model. Perhaps worth implementing Adam solver rather than stochastic gradient descent....

Epoch 1000, Loss: 0.6121805995928592 (in the above set), for Epoch 9900, Loss: 0.608805771446013. 
python testing.py -p ../output_file/outfile.txt -l ../training_data/labels.txt
